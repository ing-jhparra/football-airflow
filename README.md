<div align="center">
  <h1 align="center">
    Tutorial para utilizar Airflow en Docker utilizando Astro CLI.
          <br /> <br />
      <img src="img/astro_docker_airflow.png" alt="Airflow">
      <br />
  </h1>
</div>

# Contenido

* [Introducción](#Introducción)

* [Guia de Desarrollo](#Guia-de-Desarrollo)

* [Recursos](#Recursos)

* [Créditos](#Créditos)

# Introducción

En el mundo actual, la gestión eficiente de datos es fundamental para tomar decisiones informadas y estratégicas. Con el auge de las tecnologías de la información y el aumento exponencial de datos disponibles en la web, es crucial contar con herramientas robustas y flexibles para extraer, transformar y cargar datos de manera efectiva. En este contexto, Apache Airflow, en combinación con Astronomer, se presenta como una solución poderosa para la orquestación de flujos de trabajo complejos.

Este tutorial tiene como objetivo guiarte a través del proceso de configuración y uso de Airflow con Astronomer para llevar a cabo un proyecto de web scraping y almacenar los datos extraídos en una base de datos Snowflake. En este proyecto, aprenderás a utilizar Airflow para automatizar y gestionar tareas de scraping web, cómo transformar y procesar estos datos, y finalmente, cómo cargarlos en Snowflake, una plataforma de almacenamiento de datos en la nube de alto rendimiento.

**¿Qué es Apache Airflow?**
Apache Airflow es una plataforma de código abierto para crear, programar y monitorear flujos de trabajo complejos. Permite definir tareas y sus dependencias mediante un lenguaje de programación Python, proporcionando una interfaz web para la gestión y supervisión de los procesos.

**¿Qué es Astronomer?**
Astronomer es una plataforma que facilita la implementación, gestión y escalabilidad de Airflow en la nube. Ofrece herramientas y servicios que simplifican el despliegue y el mantenimiento de entornos de Airflow, permitiendo a los equipos de datos concentrarse en la creación de flujos de trabajo eficientes.

**¿Qué es Snowflake?**
Snowflake es una plataforma de datos en la nube que proporciona un almacenamiento y procesamiento escalable de datos. Su arquitectura basada en la nube permite manejar grandes volúmenes de datos con alta eficiencia y facilidad de integración con otras herramientas de análisis y procesamiento.

**Objetivos del Tutorial**

* Configurar un entorno de Airflow utilizando Astronomer para la ejecución de tareas.
* Implementar un proceso de web scraping para extraer datos de una fuente web específica.
* Transformar los datos obtenidos para adaptarlos a las necesidades del almacenamiento.
* Cargar los datos transformados en una base de datos Snowflake para su posterior análisis.

A lo largo de este tutorial, te guiaremos paso a paso en la configuración y ejecución de cada una de estas etapas proporcionándote las herramientas y conocimientos necesarios para completar tu proyecto con éxito. Prepárate para explorar el mundo del web scraping, la orquestación de flujos de trabajo y el análisis de datos en la nube con este completo proyecto.

# Guia de Desarrollo

1. Crear una cuenta en [Snowflake](https://www.snowflake.com/en/) con acceso gratis por 30 dias. 



# Recursos

* [Crear una cuenta gratis en Snowflake](https://www.paradigmadigital.com/dev/que-es-snowflake)

* [¿Que es Snowflake?](https://www.paradigmadigital.com/dev/que-es-snowflake/)

# Créditos
Copyright (c) 2024 [Ing. Jesús parra] parra.jesus@gmail.com
